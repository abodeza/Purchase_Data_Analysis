{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cec062",
   "metadata": {},
   "source": [
    "# This notebook is concerned with handling Item Name\n",
    "We'll be dealing with: \n",
    "- a mix of Arabic and English entries,\n",
    "- item codes/numerical values,\n",
    "- and lack of clear categorization.\n",
    "\n",
    "Approach:\n",
    "1. Break corpus into string tokens\n",
    "2. Basic normalization \n",
    "    - remove incomplete words\n",
    "    - remove fillers\n",
    "    - unnecessary chars/symbols\n",
    "3. Generate Arabic lexicon from whole words\n",
    "    - give weights based on frequency\n",
    "4. Combine scattered letters into full words based on lexicon in step 3\n",
    "5. Analyze tokens in the data \n",
    "6. Devise normalization -> reduced tokens\n",
    "7. Tokenize to numerical features\n",
    "8. Train a classifier (or classify manually) into interpretable categories\n",
    "9. Spend analysis ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b7b04",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Setup and load data](#setup-and-load-data)\n",
    "- [Handling Item Names](#Handling-Item-Names)\n",
    "    - [String tokens](#String-tokens)\n",
    "    - [Understand tokens](#Understand-tokens)\n",
    "\n",
    "- [Appendix](#Appendix)\n",
    "    - [Tests](#tests)\n",
    "        - [Trying out normalization techniques](#trying-out-normlization-techniques)\n",
    "        - [Failed attempts by ChatGPT that far took too much and had minimal improvements](#failed-attempts-by-chatgpt-that-far-took-too-much-and-had-minimal-improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a5f21",
   "metadata": {},
   "source": [
    "# Setup and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97b4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d220b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/cleaned_num.xlsx\"\n",
    "df_original = pd.read_excel(data_path)\n",
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf7675",
   "metadata": {},
   "source": [
    "# Handling Item Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67435c86",
   "metadata": {},
   "source": [
    "## String tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e8af40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry: 0    Unknown\n",
      "Name: Item Name, dtype: object\n",
      "Tokenized entry: [['Unknown']]\n",
      "---------------------------------------\n",
      "Last entry: 3148    حديد تسليح مجدول سعودي سابك 12 مم * 12 م\n",
      "Name: Item Name, dtype: object\n",
      "Tokenized entry: [['حديد', 'تسليح', 'مجدول', 'سعودي', 'سابك', '12', 'مم', '*', '12', 'م']]\n"
     ]
    }
   ],
   "source": [
    "corpus = df['Item Name']\n",
    "tokenizer = nltk.tokenize.NLTKWordTokenizer()\n",
    "\n",
    "tokens_head = tokenizer.tokenize_sents(corpus[0:1])\n",
    "tokens_tail = tokenizer.tokenize_sents(corpus[-2:-1])\n",
    "\n",
    "print(\"First entry:\", corpus[0:1])\n",
    "print(\"Tokenized entry:\", tokens_head)\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Last entry:\", corpus[-2:-1])\n",
    "print(\"Tokenized entry:\", tokens_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff7676",
   "metadata": {},
   "source": [
    "## Understand tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefdc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9893fb3a",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f535801",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e53c7c",
   "metadata": {},
   "source": [
    "### Trying out normlization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51677563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﻣ ﺎ ﺳ و ﺭ ﺓ ﺍ ﺳ و د 6 ﺑ و ﺻ ﺔ * 7 ﻣ ﻠ ﻡ * 12 ﻣ ﺗ ﺭ'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ﻣ ﺎ ﺳ و ﺭ ﺓ ﺍ ﺳ و د 6 ﺑ و ﺻ ﺔ * 7 ﻣ ﻠ ﻡ * 12 ﻣ ﺗ ﺭ\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82779055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'م ا س و ر ة ا س و د 6 ب و ص ة * 7 م ل م * 12 م ت ر'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This does a good job in standardizing the characters\n",
    "import unicodedata\n",
    "result = unicodedata.normalize(\"NFKC\", text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b0380e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﻣ ﺎ ﺳ و ﺭ ﺓ ﺍ ﺳ و د 6 ﺑ و ﺻ ﺔ * 7 ﻣ ﻠ ﻡ * 12 ﻣ ﺗ ﺭ'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "result = re.sub(r'[\\u200B-\\u200F\\u202A-\\u202E\\u2066-\\u2069]', '', text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e8a1222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ﻣ ﺎ ﺳ و ﺭ ﺓ ﺍ ﺳ و د 6 ﺑ و ﺻ ﺔ * 7 ﻣ ﻠ ﻡ * 12 ﻣ ﺗ ﺭ'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = re.sub(r'\\s+', ' ', text).strip()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1472dbe",
   "metadata": {},
   "source": [
    "None of the techniques above effictively resolves the issue of spaces and word recognition in text...\n",
    "\n",
    "We'll try out Camel-tools, I've had a little experience with it before, but never worked with data this messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7ea6297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camel_tools.utils.normalize as norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88cdce14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'م ا س و ر ة ا س و د 6 ب و ص ة * 7 م ل م * 12 م ت ر'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.normalize_unicode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9bd08",
   "metadata": {},
   "source": [
    "### Failed attempts by ChatGPT that far took too much and had minimal improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3291fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تیوبفارغمربع 80 × 80 × 5 مل 6 متر\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_maksura_ar\n",
    "\n",
    "# Unicode ranges that cover Arabic letters (not presentation forms)\n",
    "_AR = r'\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF'\n",
    "\n",
    "def normalize_mixed_arabic(s: str) -> str:\n",
    "    # 1) Canonical/compatibility normalization\n",
    "    s = normalize_unicode(s, compatibility=True)\n",
    "    # 2) Normalize alef maqsura, optional but useful\n",
    "    s = normalize_alef_maksura_ar(s)\n",
    "\n",
    "    # 3) Strip zero-width/bidi formatting chars\n",
    "    s = re.sub(r'[\\u200B-\\u200F\\u202A-\\u202E\\u2066-\\u2069]', '', s)\n",
    "\n",
    "    # 4) Normalize common operators and pad them\n",
    "    #    Treat *, ×, x as multiplication; surround with single spaces\n",
    "    s = re.sub(r'\\s*([*xX×])\\s*', r' × ', s)\n",
    "\n",
    "    # 5) **Key step**: remove spaces *only* between Arabic letters\n",
    "    #    This stitches scattered letters back into words\n",
    "    s = re.sub(fr'(?<=[{_AR}])\\s+(?=[{_AR}])', '', s)\n",
    "\n",
    "    # 6) Collapse remaining excessive whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# Demo on your string\n",
    "sample = \"ﺗ ﯾ و ﺏ ﻓ ﺎ ﺭ ﻍ ﻣ ﺭ ﺑ ﻊ 80 * 80 * 5 ﻣ ﻝ 6 ﻣ ﺗ ﺭ\"\n",
    "print(normalize_mixed_arabic(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e599fd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['تیوبفارغمربع', '80', '×', '80', '×', '5', 'مل', '6', 'متر']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "\n",
    "# 1) choose a pretrained analyzer/disambiguator (MSA shown)\n",
    "mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "# 2) after your intra-letter stitching + spacing heuristics:\n",
    "clean = \"ﺗ ﯾ و ﺏ ﻓ ﺎ ﺭ ﻍ ﻣ ﺭ ﺑ ﻊ 80 * 80 * 5 ﻣ ﻝ 6 ﻣ ﺗ ﺭ\"#\"تیوبفارغمربع 80 × 80 × 5 مل 6 متر\"\n",
    "\n",
    "# 3) whitespace/punct tokenization (required by CAMeL’s morph tokenizer)\n",
    "words = simple_word_tokenize(normalize_mixed_arabic(clean))\n",
    "\n",
    "# 4) morphological tokenization (e.g., ATB; set split=True to get separate tokens)\n",
    "mtok = MorphologicalTokenizer(disambiguator=mle, scheme='atbtok', split=True)\n",
    "segmented = mtok.tokenize(words)\n",
    "segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb721526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تیو بفا ر غمر بع 80 * 80 * 5 مل 6 متر\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from camel_tools.utils.normalize import (\n",
    "    normalize_alef_ar,           # unify alef variants: أ إ آ -> ا\n",
    "    normalize_alef_maksura_ar,   # ى -> ي\n",
    "    normalize_teh_marbuta_ar     # ة -> ه or keep ة depending on needs\n",
    ")\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "\n",
    "# Arabic block after NFKC\n",
    "ARABIC_RANGE = r\"\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\"\n",
    "AR_LETTER = f\"[{ARABIC_RANGE}]\"\n",
    "DIGIT = r\"[0-9\\u0660-\\u0669]\"  # Western + Arabic-Indic\n",
    "\n",
    "def to_ascii_digits(s: str) -> str:\n",
    "    # map Arabic-Indic digits to ASCII\n",
    "    trans = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "    return s.translate(trans)\n",
    "\n",
    "def normalize_ar_text(s: str, keep_diacritics=False):\n",
    "    # 1) collapse presentation forms, compatibility, widths, etc.\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "    # 2) remove tatweel and bidi/formatting chars\n",
    "    s = s.replace(\"\\u0640\", \"\")  # tatweel\n",
    "    s = re.sub(r\"[\\u200c\\u200d\\u200e\\u200f\\u061c]\", \"\", s)  # ZWNJ/ZWJ/LRM/RLM/ALM\n",
    "\n",
    "    # 3) unify common Arabic variants\n",
    "    s = normalize_alef_ar(s)\n",
    "    s = normalize_alef_maksura_ar(s)\n",
    "    s = normalize_teh_marbuta_ar(s)\n",
    "\n",
    "    # 4) optional: strip diacritics (recommended for noisy sources)\n",
    "    if not keep_diacritics:\n",
    "        s = dediac_ar(s)\n",
    "\n",
    "    # 5) digits to ASCII for easier heuristics\n",
    "    s = to_ascii_digits(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def fix_spaced_glyphs(s: str):\n",
    "    s = normalize_ar_text(s)\n",
    "\n",
    "    # --- spacing repair heuristics ---\n",
    "\n",
    "    # A) collapse spaces inserted between Arabic letters (re-form words)\n",
    "    # remove spaces when both sides are Arabic letters\n",
    "    s = re.sub(fr\"(?<={AR_LETTER})\\s+(?={AR_LETTER})\", \"\", s)\n",
    "\n",
    "    # B) ensure spaces around math operators and 'x'\n",
    "    s = re.sub(r\"\\s*([*+\\-/x×])\\s*\", r\" \\1 \", s)\n",
    "\n",
    "    # C) split between letters and digits (both directions)\n",
    "    s = re.sub(fr\"(?<={AR_LETTER})(?={DIGIT})\", \" \", s)\n",
    "    s = re.sub(fr\"(?<={DIGIT})(?={AR_LETTER})\", \" \", s)\n",
    "\n",
    "    # D) heuristic: insert a space when a right-joining (non-connecting-left) letter\n",
    "    # is followed by another letter (helps recover word boundaries)\n",
    "    non_joiners = \"ادذرزوةى\"  # letters that do not connect to the following letter\n",
    "    s = re.sub(fr\"(?<=[{non_joiners}])(?={AR_LETTER})\", \" \", s)\n",
    "\n",
    "    # E) normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "sample = \"ﺗ ﯾ و ﺏ ﻓ ﺎ ﺭ ﻍ ﻣ ﺭ ﺑ ﻊ 80 * 80 * 5 ﻣ ﻝ 6 ﻣ ﺗ ﺭ\"\n",
    "print(fix_spaced_glyphs(sample))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
